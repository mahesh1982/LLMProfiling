{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ac3e5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.10.0-cp314-cp314-macosx_14_0_arm64.whl.metadata (31 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached setuptools-81.0.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Using cached fsspec-2026.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading markupsafe-3.0.3-cp314-cp314-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Downloading torch-2.10.0-cp314-cp314-macosx_14_0_arm64.whl (79.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached fsspec-2026.2.0-py3-none-any.whl (202 kB)\n",
      "Using cached networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached filelock-3.20.3-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading markupsafe-3.0.3-cp314-cp314-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached setuptools-81.0.0-py3-none-any.whl (1.1 MB)\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, setuptools, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/10\u001b[0m [torch]m 9/10\u001b[0m [torch]ck]s]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 filelock-3.20.3 fsspec-2026.2.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.6.1 setuptools-81.0.0 sympy-1.14.0 torch-2.10.0 typing-extensions-4.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "335ad109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-5.1.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting huggingface-hub<2.0,>=1.3.0 (from transformers)\n",
      "  Using cached huggingface_hub-1.4.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Downloading numpy-2.4.2-cp314-cp314-macosx_14_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.14/site-packages (from transformers) (26.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading pyyaml-6.0.3-cp314-cp314-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2026.1.15-cp314-cp314-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Collecting typer-slim (from transformers)\n",
      "  Using cached typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.3-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.14/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.14/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2026.2.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in ./.venv/lib/python3.14/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Using cached anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Using cached certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting click>=8.0.0 (from typer-slim->transformers)\n",
      "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Using cached transformers-5.1.0-py3-none-any.whl (10.3 MB)\n",
      "Using cached huggingface_hub-1.4.1-py3-none-any.whl (553 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl (3.0 MB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading numpy-2.4.2-cp314-cp314-macosx_14_0_arm64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyyaml-6.0.3-cp314-cp314-macosx_11_0_arm64.whl (173 kB)\n",
      "Downloading regex-2026.1.15-cp314-cp314-macosx_11_0_arm64.whl (288 kB)\n",
      "Using cached safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl (447 kB)\n",
      "Using cached tqdm-4.67.3-py3-none-any.whl (78 kB)\n",
      "Using cached anyio-4.12.1-py3-none-any.whl (113 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
      "Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Installing collected packages: tqdm, shellingham, safetensors, regex, pyyaml, numpy, idna, hf-xet, h11, click, certifi, typer-slim, httpcore, anyio, httpx, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/18\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed anyio-4.12.1 certifi-2026.1.4 click-8.3.1 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-1.4.1 idna-3.11 numpy-2.4.2 pyyaml-6.0.3 regex-2026.1.15 safetensors-0.7.0 shellingham-1.5.4 tokenizers-0.22.2 tqdm-4.67.3 transformers-5.1.0 typer-slim-0.21.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e645e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maheswarareddyp/Documents/Courses/Udacity/LLMProfiling/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.profiler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f49d79c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 148/148 [00:00<00:00, 2415.34it/s, Materializing param=transformer.wte.weight]             \n",
      "\u001b[1mGPT2LMHeadModel LOAD REPORT\u001b[0m from: gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f0d237b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13245d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input prompt: The future of artificial intelligence is\n",
      "Task: Generate 50 new tokens.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The future of artificial intelligence is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "num_new_tokens_to_generate = 50\n",
    "\n",
    "print(f\"Input prompt: {prompt}\")\n",
    "print(f\"Task: Generate {num_new_tokens_to_generate} new tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10df2994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Profiling on CPU ---\n",
      "running inference on CPU and capturing profile...\n",
      "CPU inference completed in 0.95 seconds.\n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        cpu_inference         9.57%      85.621ms       100.00%     894.735ms     894.735ms             1  \n",
      "                                          aten::addmm        58.04%     519.304ms        58.43%     522.813ms     217.839us          2400  \n",
      "                                         aten::linear         0.05%     415.837us        20.01%     178.999ms       3.580ms            50  \n",
      "                                         aten::matmul         0.06%     510.134us        19.94%     178.372ms       3.567ms            50  \n",
      "                                             aten::mm        19.86%     177.717ms        19.86%     177.719ms       3.554ms            50  \n",
      "                   aten::scaled_dot_product_attention         0.14%       1.249ms         3.29%      29.468ms      49.113us           600  \n",
      "    aten::_scaled_dot_product_flash_attention_for_cpu         2.86%      25.612ms         3.15%      28.219ms      47.032us           600  \n",
      "                                           aten::tanh         2.40%      21.463ms         2.40%      21.463ms      35.772us           600  \n",
      "                                            aten::cat         1.89%      16.874ms         1.97%      17.636ms      13.566us          1300  \n",
      "                                     aten::layer_norm         0.09%     794.559us         0.71%       6.320ms       5.056us          1250  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 894.735ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Profiling on CPU ---\")\n",
    "model.to(\"cpu\")\n",
    "inputs_cpu = {k: v.to(\"cpu\") for k, v in inputs.items()}\n",
    "\n",
    "def run_cpu_inference(input_data, max_tokens):\n",
    "    with torch.no_grad():\n",
    "        model.generate(\n",
    "            input_ids=input_data[\"input_ids\"],\n",
    "            attention_mask=input_data[\"attention_mask\"],\n",
    "            max_new_tokens=max_tokens,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "        \n",
    "print(\"running inference on CPU and capturing profile...\")\n",
    "start_time_cpu = time.time()\n",
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU],\n",
    "    record_shapes=False,\n",
    "    profile_memory=False,\n",
    ") as prof_cpu:\n",
    "    with torch.profiler.record_function(\"cpu_inference\"):\n",
    "        run_cpu_inference(inputs_cpu, num_new_tokens_to_generate)\n",
    "end_time_cpu = time.time()\n",
    "print(f\"CPU inference completed in {end_time_cpu - start_time_cpu:.2f} seconds.\")\n",
    "print(prof_cpu.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc25350c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "\n",
      "--- Profiling on MPS (Apple GPU) ---\n",
      "Inference completed in 0.32 seconds.\n",
      "----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                       mps_inference        15.20%      41.247ms       100.00%     271.288ms     271.288ms             1  \n",
      "                                          aten::item         0.07%     196.226us        38.70%     104.980ms     116.257us           903  \n",
      "                                    aten::is_nonzero         0.02%      56.588us        38.65%     104.854ms     689.830us           152  \n",
      "                           aten::_local_scalar_dense        38.60%     104.706ms        38.62%     104.783ms     116.039us           903  \n",
      "                                            aten::to         0.15%     403.053us        15.45%      41.925ms      12.786us          3279  \n",
      "                                      aten::_to_copy         0.29%     777.795us        15.31%      41.522ms      16.431us          2527  \n",
      "                                         aten::copy_        15.09%      40.927ms        15.09%      40.930ms      15.498us          2641  \n",
      "                                         aten::addmm        12.92%      35.049ms        13.18%      35.766ms      14.903us          2400  \n",
      "                                           aten::pow         2.93%       7.955ms         3.00%       8.134ms      13.557us           600  \n",
      "                                           aten::mul         1.39%       3.778ms         1.96%       5.314ms       2.126us          2500  \n",
      "----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 271.288ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# Check for MPS (Apple GPU)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "inputs_device = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "def run_mps_inference(input_data, max_tokens):\n",
    "    with torch.no_grad():\n",
    "        if device == \"mps\":\n",
    "            torch.mps.synchronize()  # ensure previous ops complete\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_data[\"input_ids\"],\n",
    "            attention_mask=input_data[\"attention_mask\"],\n",
    "            max_new_tokens=max_tokens,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        if device == \"mps\":\n",
    "            torch.mps.synchronize()\n",
    "        return outputs\n",
    "\n",
    "print(\"\\n--- Profiling on MPS (Apple GPU) ---\" if device == \"mps\" else \"\\n--- CPU only ---\")\n",
    "\n",
    "# Warmup\n",
    "run_mps_inference(inputs_device, num_new_tokens_to_generate)\n",
    "\n",
    "# Profile\n",
    "start = time.time()\n",
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU],  # MPS does not support CUDA profiling\n",
    "    record_shapes=False,\n",
    "    profile_memory=False,\n",
    ") as prof:\n",
    "    with torch.profiler.record_function(\"mps_inference\"):\n",
    "        run_mps_inference(inputs_device, num_new_tokens_to_generate)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Inference completed in {end - start:.2f} seconds.\")\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
